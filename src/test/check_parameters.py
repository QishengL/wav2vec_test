import torch
from transformers import AutoModelForPreTraining,AutoConfig,Wav2Vec2ForPreTraining,AutoModelForCTC
from collections import defaultdict

def analyze_pretrained_model_freeze_status(model_path="facebook/wav2vec2-xls-r-300m"):
    """
    åˆ†æé¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°å†»ç»“çŠ¶æ€
    """
    print("=" * 80)
    print(f"åˆ†ææ¨¡å‹: {model_path}")
    print("=" * 80)
    
    # åŠ è½½æ¨¡å‹
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    model = AutoModelForPreTraining.from_pretrained(model_path)
    print("æ¨¡å‹åŠ è½½å®Œæˆ!\n")
    
    # åˆ†æå‚æ•°çŠ¶æ€
    total_params = 0
    trainable_params = 0
    frozen_params = 0
    
    # æŒ‰æ¨¡å—åˆ†ç±»ç»Ÿè®¡
    module_stats = defaultdict(lambda: {'trainable': 0, 'frozen': 0, 'count': 0})
    
    print("å‚æ•°å†»ç»“çŠ¶æ€åˆ†æ:")
    print("-" * 80)
    
    for name, param in model.named_parameters():
        total_params += param.numel()
        
        # æå–æ¨¡å—åç§°ï¼ˆå‰ä¸¤çº§ï¼‰
        parts = name.split('.')
        if len(parts) >= 2:
            module_name = '.'.join(parts[:2])  # å¦‚ 'wav2vec2.feature_extractor'
        else:
            module_name = name
        
        if param.requires_grad:
            trainable_params += param.numel()
            module_stats[module_name]['trainable'] += param.numel()
        else:
            frozen_params += param.numel()
            module_stats[module_name]['frozen'] += param.numel()
        
        module_stats[module_name]['count'] += 1
    
    # æ‰“å°æ€»ä½“ç»Ÿè®¡
    print(f"\næ€»ä½“ç»Ÿè®¡:")
    print(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/total_params*100:.4f}%)")
    print(f"å†»ç»“å‚æ•°: {frozen_params:,} ({frozen_params/total_params*100:.4f}%)")
    
    # æ‰“å°å„æ¨¡å—è¯¦ç»†ç»Ÿè®¡
    print(f"\nå„æ¨¡å—è¯¦ç»†ç»Ÿè®¡:")
    print("-" * 80)
    
    # æŒ‰æ¨¡å—æ€»å‚æ•°æ’åº
    sorted_modules = sorted(module_stats.items(), 
                           key=lambda x: x[1]['trainable'] + x[1]['frozen'], 
                           reverse=True)
    
    for module_name, stats in sorted_modules:
        module_total = stats['trainable'] + stats['frozen']
        if module_total == 0:
            continue
            
        trainable_percent = stats['trainable'] / module_total * 100
        param_count = stats['count']
        
        status = "å¯è®­ç»ƒ" if stats['trainable'] > 0 else "å†»ç»“"
        
        print(f"{module_name:.<40} {status:.<8} "
              f"{stats['trainable']:>10,} / {module_total:>10,} "
              f"({trainable_percent:>6.2f}%) "
              f"[{param_count:>2} params]")
    
    return model, total_params, trainable_params, frozen_params

def check_specific_layers(model):
    """
    æ£€æŸ¥ç‰¹å®šå±‚çš„å†»ç»“çŠ¶æ€
    """
    print(f"\nç‰¹å®šå±‚æ£€æŸ¥:")
    print("-" * 80)
    
    # å®šä¹‰ä¸€äº›å…³é”®å±‚çš„å…³é”®è¯
    key_layers = [
        'feature_extractor', 'feature_projection', 'encoder', 
        'quantizer', 'project_q', 'project_hid', 'masked_spec_embed',
        'final_proj', 'lm_head'
    ]
    
    found_layers = []
    
    for name, param in model.named_parameters():
        for key in key_layers:
            if key in name:
                status = "å¯è®­ç»ƒ" if param.requires_grad else "å†»ç»“"
                found_layers.append((name, status, param.numel()))
                break
    
    # æ‰“å°æ‰¾åˆ°çš„å…³é”®å±‚
    for name, status, numel in sorted(found_layers)[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ª
        print(f"{name:.<60} {status:.<8} {numel:>10,}")
    
    if len(found_layers) > 20:
        print(f"... è¿˜æœ‰ {len(found_layers) - 20} ä¸ªå…³é”®å±‚")

def analyze_model_structure(model):
    """
    åˆ†ææ¨¡å‹ç»“æ„
    """
    print(f"\næ¨¡å‹ç»“æ„åˆ†æ:")
    print("-" * 80)
    
    # è·å–æ¨¡å‹çš„ä¸»è¦ç»„ä»¶
    model_keys = list(model.state_dict().keys())
    
    # ç»Ÿè®¡å„ç»„ä»¶æ•°é‡
    component_count = defaultdict(int)
    for key in model_keys:
        first_part = key.split('.')[0]
        component_count[first_part] += 1
    
    print("ä¸»è¦ç»„ä»¶:")
    for component, count in sorted(component_count.items()):
        print(f"  {component}: {count} ä¸ªå‚æ•°å¼ é‡")
    
    # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹å‚æ•°
    print(f"\nå‚æ•°ç¤ºä¾‹ (å‰10ä¸ª):")
    print("-" * 40)
    for i, (name, param) in enumerate(list(model.named_parameters())[:10]):
        status = "å¯è®­ç»ƒ" if param.requires_grad else "å†»ç»“"
        print(f"{i+1:2}. {name}")
        print(f"    å½¢çŠ¶: {list(param.shape)}, çŠ¶æ€: {status}")
        print(f"    å‚æ•°æ•°é‡: {param.numel():,}")

def compare_before_after_lora(model):
    """
    æ¯”è¾ƒåº”ç”¨LoRAå‰åçš„å‚æ•°çŠ¶æ€
    """
    print(f"\nLoRAåº”ç”¨å‰åå¯¹æ¯”:")
    print("-" * 80)
    
    # è®°å½•åŸå§‹çŠ¶æ€
    original_state = {}
    for name, param in model.named_parameters():
        original_state[name] = {
            'requires_grad': param.requires_grad,
            'numel': param.numel()
        }
    
    print("åŸå§‹æ¨¡å‹çŠ¶æ€:")
    total_original = sum(1 for name, param in model.named_parameters() if param.requires_grad)
    print(f"å¯è®­ç»ƒå‚æ•°å¼ é‡: {total_original}/{len(original_state)}")
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ åº”ç”¨LoRAçš„ä»£ç ï¼Œç„¶åæ¯”è¾ƒçŠ¶æ€å˜åŒ–
    print("\nåº”ç”¨LoRAå:")
    print("æ‰€æœ‰åŸå§‹å‚æ•°åº”è¯¥è¢«å†»ç»“ï¼Œåªæœ‰LoRAé€‚é…å™¨å‚æ•°å¯è®­ç»ƒ")

# ä¸»æ‰§è¡Œå‡½æ•°
def main():
    """
    ä¸»åˆ†æå‡½æ•°
    """
    model_path = "facebook/wav2vec2-large-xlsr-53"
    
    try:
        # 1. åˆ†æé¢„è®­ç»ƒæ¨¡å‹çš„å†»ç»“çŠ¶æ€
        model, total, trainable, frozen = analyze_pretrained_model_freeze_status(model_path)
        
        # 2. æ£€æŸ¥ç‰¹å®šå±‚
        check_specific_layers(model)
        
        # 3. åˆ†ææ¨¡å‹ç»“æ„
        analyze_model_structure(model)
        
        # 4. LoRAå‰åå¯¹æ¯”è¯´æ˜
        compare_before_after_lora(model)
        
        print(f"\n" + "=" * 80)
        print("åˆ†ææ€»ç»“:")
        print(f"æ¨¡å‹: {model_path}")
        print(f"é»˜è®¤æƒ…å†µä¸‹ï¼Œ{trainable/total*100:.4f}% çš„å‚æ•°æ˜¯å¯è®­ç»ƒçš„")
        print(f"è¿™æ„å‘³ç€åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œé»˜è®¤æ‰€æœ‰å‚æ•°éƒ½æ˜¯å¯è®­ç»ƒçš„")
        print(f"åº”ç”¨LoRAåï¼Œæ‰€æœ‰åŸå§‹å‚æ•°å°†è¢«å†»ç»“ï¼Œåªè®­ç»ƒLoRAé€‚é…å™¨")
        print("=" * 80)
        
        return model
        
    except Exception as e:
        print(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return None

# å¿«é€Ÿæ£€æŸ¥å‡½æ•°
def quick_freeze_check(model_path="facebook/wav2vec2-xls-r-300m"):
    """
    å¿«é€Ÿæ£€æŸ¥æ¨¡å‹çš„å†»ç»“çŠ¶æ€
    """
    print("å¿«é€Ÿæ£€æŸ¥æ¨¡å‹å†»ç»“çŠ¶æ€...")
    
    model = AutoModelForPreTraining.from_pretrained(model_path)
    
    trainable_count = 0
    total_count = 0
    trainable_params = 0
    total_params = 0
    
    for name, param in model.named_parameters():
        total_count += 1
        total_params += param.numel()
        if param.requires_grad:
            trainable_count += 1
            trainable_params += param.numel()
    
    print(f"å‚æ•°å¼ é‡æ•°é‡: {trainable_count}/{total_count} å¯è®­ç»ƒ")
    print(f"å‚æ•°æ•°é‡: {trainable_params:,}/{total_params:,} å¯è®­ç»ƒ")
    print(f"å¯è®­ç»ƒæ¯”ä¾‹: {trainable_params/total_params*100:.4f}%")
    
    return model

def quick_freeze_feature_check(model_path="facebook/wav2vec2-xls-r-300m"):
    """
    å¿«é€Ÿæ£€æŸ¥freeze_feature_encoderçš„å½±å“
    """
    print("ğŸ” å¿«é€Ÿæ£€æŸ¥ freeze_feature_encoder å½±å“")
    print("-" * 60)
    
    # å†»ç»“é…ç½®
    config_frozen = AutoConfig.from_pretrained(model_path, freeze_feature_encoder=True)
    model_frozen = AutoModelForPreTraining.from_pretrained(model_path, config=config_frozen)
    
    frozen_feature_params = 0
    frozen_total_params = 0
    frozen_trainable_params = 0  # æ–°å¢ï¼šç»Ÿè®¡å®é™…å¯è®­ç»ƒå‚æ•°
    
    for name, param in model_frozen.named_parameters():
        frozen_total_params += param.numel()
        if param.requires_grad:
            frozen_trainable_params += param.numel()  # ç»Ÿè®¡å®é™…å¯è®­ç»ƒçš„å‚æ•°
        
        if 'feature_extractor' in name:
            frozen_feature_params += param.numel()
    
    # æœªå†»ç»“é…ç½®
    config_unfrozen = AutoConfig.from_pretrained(model_path, freeze_feature_encoder=False)
    model_unfrozen = AutoModelForPreTraining.from_pretrained(model_path, config=config_unfrozen)
    
    unfrozen_feature_params = 0
    unfrozen_total_params = 0
    unfrozen_trainable_params = 0  # æ–°å¢ï¼šç»Ÿè®¡å®é™…å¯è®­ç»ƒå‚æ•°
    
    for name, param in model_unfrozen.named_parameters():
        unfrozen_total_params += param.numel()
        if param.requires_grad:
            unfrozen_trainable_params += param.numel()
        
        if 'feature_extractor' in name:
            unfrozen_feature_params += param.numel()
    
    print(f"freeze_feature_encoder=True:")
    print(f"  Feature Encoderå‚æ•°: {frozen_feature_params:,}")
    print(f"  æ€»å‚æ•°: {frozen_total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {frozen_trainable_params:,}")  # æ˜¾ç¤ºå®é™…å¯è®­ç»ƒå‚æ•°
    print(f"  å¯è®­ç»ƒæ¯”ä¾‹: {frozen_trainable_params/frozen_total_params*100:.2f}%")
    
    print(f"\nfreeze_feature_encoder=False:")
    print(f"  Feature Encoderå‚æ•°: {unfrozen_feature_params:,}")
    print(f"  æ€»å‚æ•°: {unfrozen_total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {unfrozen_trainable_params:,}")  # æ˜¾ç¤ºå®é™…å¯è®­ç»ƒå‚æ•°
    print(f"  å¯è®­ç»ƒæ¯”ä¾‹: {unfrozen_trainable_params/unfrozen_total_params*100:.2f}%")
    
    # è¯¦ç»†æ£€æŸ¥feature_extractorçš„å†»ç»“çŠ¶æ€
    print(f"\nğŸ” è¯¦ç»†æ£€æŸ¥feature_extractorå†»ç»“çŠ¶æ€:")
    print("-" * 50)
    
    print("freeze_feature_encoder=True æ—¶çš„feature_extractorå±‚:")
    feature_frozen_count = 0
    for name, param in model_frozen.named_parameters():
        if 'feature_extractor' in name:
            status = "å†»ç»“" if not param.requires_grad else "å¯è®­ç»ƒ"
            print(f"  {name}: {status}")
            if not param.requires_grad:
                feature_frozen_count += 1
    
    print(f"\nfreeze_feature_encoder=False æ—¶çš„feature_extractorå±‚:")
    feature_unfrozen_count = 0
    for name, param in model_unfrozen.named_parameters():
        if 'feature_extractor' in name:
            status = "å†»ç»“" if not param.requires_grad else "å¯è®­ç»ƒ"
            print(f"  {name}: {status}")
            if param.requires_grad:
                feature_unfrozen_count += 1
    
    print(f"\nğŸ“Š å†»ç»“ç»Ÿè®¡:")
    print(f"  å†»ç»“é…ç½®ä¸‹ï¼Œ{feature_frozen_count} ä¸ªfeature_extractorå±‚è¢«å†»ç»“")
    print(f"  æœªå†»ç»“é…ç½®ä¸‹ï¼Œ{feature_unfrozen_count} ä¸ªfeature_extractorå±‚å¯è®­ç»ƒ")
def correct_freeze_solution(model_path="facebook/wav2vec2-large-xlsr-53"):
    """
    æ­£ç¡®çš„feature_encoderå†»ç»“æ–¹æ³•
    """
    print("ğŸ”§ æ­£ç¡®çš„å†»ç»“æ–¹æ³•")
    print("=" * 60)
    
    # æ–¹æ³•1ï¼šä½¿ç”¨æ¨¡å‹å†…ç½®çš„å†»ç»“æ–¹æ³•
    print("æ–¹æ³•1: ä½¿ç”¨æ¨¡å‹å†…ç½®æ–¹æ³•")
    model = AutoModelForCTC.from_pretrained(model_path)
    
    # è°ƒç”¨æ¨¡å‹çš„å†…ç½®å†»ç»“æ–¹æ³•
    #model.freeze_feature_encoder()
    
    trainable_count = 0
    total_count = 0
    trainable_params = 0
    total_params = 0
    
    for name, param in model.named_parameters():
        total_count += 1
        total_params += param.numel()
        if param.requires_grad:
            trainable_count += 1
            trainable_params += param.numel()
    
    print(f"å‚æ•°å¼ é‡æ•°é‡: {trainable_count}/{total_count} å¯è®­ç»ƒ")
    print(f"å‚æ•°æ•°é‡: {trainable_params:,}/{total_params:,} å¯è®­ç»ƒ")
    print(f"å¯è®­ç»ƒæ¯”ä¾‹: {trainable_params/total_params*100:.4f}%")
    
    return model


if __name__ == "__main__":
    # è¿è¡Œå®Œæ•´åˆ†æ
    model = main()
    
    print(f"\n" + "=" * 80)
    print("å¿«é€Ÿæ£€æŸ¥:")
    print("=" * 80)
    correct_freeze_solution()