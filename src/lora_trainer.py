from peft import LoraConfig, get_peft_model, TaskType,PeftModel
import torch
from transformers import AutoModelForCTC, AutoProcessor,Trainer,Wav2Vec2ForCTC
from datasets import DatasetDict
import evaluate
from collator import DataCollatorCTCWithPadding
import wandb
import json


class MyWav2Vec2ForCTC(Wav2Vec2ForCTC):
    def get_input_embeddings(self):
        return self.lm_head
    
    def set_input_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def resize_token_embeddings(self, new_num_tokens = None):
        """
        è°ƒæ•´è¯æ±‡è¡¨å¤§å°
        """
        if new_num_tokens is None:
            return self.lm_head
        
        old_lm_head = self.lm_head
        old_num_tokens = old_lm_head.out_features
        
        if new_num_tokens == old_num_tokens:
            return self.lm_head
        
        # åˆ›å»ºæ–°çš„çº¿æ€§å±‚
        new_lm_head = torch.nn.Linear(
            old_lm_head.in_features, 
            new_num_tokens, 
            bias=old_lm_head.bias is not None
        )
        
        # å¤åˆ¶æƒé‡
        with torch.no_grad():
            if new_num_tokens > old_num_tokens:
                new_lm_head.weight.data[:old_num_tokens] = old_lm_head.weight.data
                # æ–° token éšæœºåˆå§‹åŒ–
                std_dev = 0.02
                new_lm_head.weight.data[old_num_tokens:] = torch.randn(
                    new_num_tokens - old_num_tokens, old_lm_head.in_features
                ) * std_dev
                
                if old_lm_head.bias is not None:
                    new_lm_head.bias.data[:old_num_tokens] = old_lm_head.bias.data
                    new_lm_head.bias.data[old_num_tokens:] = 0
            else:
                new_lm_head.weight.data = old_lm_head.weight.data[:new_num_tokens]
                if old_lm_head.bias is not None:
                    new_lm_head.bias.data = old_lm_head.bias.data[:new_num_tokens]
        
        self.lm_head = new_lm_head
        self.config.vocab_size = new_num_tokens

    def reinit_token_embeddings(self, new_num_tokens = None):
        """
        å®Œå…¨é‡æ–°è®¾ç½®è¯æ±‡è¡¨ï¼Œåªä¿ç•™ç‰¹æ®Štoken
        """
        if new_num_tokens is None:
            return self.lm_head
        
        old_lm_head = self.lm_head
        old_num_tokens = old_lm_head.out_features
        
        print(f"=== Complete Vocabulary Reset ===")
        print(f"Old vocab size: {old_num_tokens}")
        print(f"New vocab size: {new_num_tokens}")
        
        # åˆ›å»ºæ–°çš„çº¿æ€§å±‚
        new_lm_head = torch.nn.Linear(
            old_lm_head.in_features, 
            new_num_tokens, 
            bias=old_lm_head.bias is not None
        )
        
        print(f"New lm_head shape: {new_lm_head.weight.shape}")
        
        # å®Œå…¨é‡æ–°åˆå§‹åŒ–æ‰€æœ‰æƒé‡
        with torch.no_grad():
            # æ‰€æœ‰tokenéƒ½é‡æ–°åˆå§‹åŒ–
            std_dev = 0.02
            new_lm_head.weight.data = torch.randn(
                new_num_tokens, old_lm_head.in_features
            ) * std_dev
            
            if old_lm_head.bias is not None:
                new_lm_head.bias.data.zero_()
        
        self.lm_head = new_lm_head
        self.config.vocab_size = new_num_tokens
        
        print(f"âœ… Complete vocabulary reset completed")
        print(f"âœ… All {new_num_tokens} tokens are newly initialized")
        

class MultiLanguageEvaluationTrainer(Trainer):
    def __init__(self, *args, language_column="language", languages=None, **kwargs):
        #print("ğŸ”§ MultiLanguageEvaluationTrainer.__init__ è¢«è°ƒç”¨")
        super().__init__(*args, **kwargs)
        self.language_column = language_column
        self.languages = languages or []
        #print(f"ğŸ”§ åˆå§‹åŒ–å‚æ•°: language_column={language_column}, languages={languages}")
    
    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix="eval"):
        #print("ğŸ¯ğŸ¯ğŸ¯ MultiLanguageEvaluationTrainer.evaluate è¢«è°ƒç”¨!")

        if eval_dataset is None:
            eval_dataset = self.eval_dataset

        #print(f"ğŸ¯ eval_dataset: {eval_dataset}")
        #print(f"ğŸ¯ metric_key_prefix: {metric_key_prefix}")
        
        # å¦‚æœæœ‰å¤šè¯­è¨€æ•°æ®ï¼Œç›´æ¥è¿›è¡Œå¤šè¯­è¨€è¯„ä¼°å¹¶è®¡ç®—åŠ æƒå¹³å‡
        if eval_dataset is not None and self.language_column in eval_dataset.column_names:
            #print("ğŸ¯ ä½¿ç”¨å¤šè¯­è¨€åŠ æƒè¯„ä¼°...")
            metrics = self._evaluate_with_weighted_average(eval_dataset, ignore_keys, metric_key_prefix)
        else:
            # æ²¡æœ‰è¯­è¨€ä¿¡æ¯ï¼Œå›é€€åˆ°é»˜è®¤è¯„ä¼°
            #print("ğŸ¯ ä½¿ç”¨é»˜è®¤è¯„ä¼°...")
            metrics = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)
        
        #print(f"ğŸ¯ æœ€ç»ˆè¿”å›çš„ metrics: {metrics}")
        return metrics
    
    def _evaluate_with_weighted_average(self, eval_dataset, ignore_keys, metric_key_prefix):
        """ä½¿ç”¨åŠ æƒå¹³å‡è¿›è¡Œå¤šè¯­è¨€è¯„ä¼°"""
        if not self.languages:
            self.languages = list(set(eval_dataset[self.language_column]))
        
        #print(f"ğŸ¯ æ£€æµ‹åˆ°çš„è¯­è¨€: {self.languages}")
        
        all_lang_metrics = {}
        total_samples = 0
        
        # åˆ†åˆ«è¯„ä¼°æ¯ç§è¯­è¨€
        for lang_name in self.languages:
            #print(f"ğŸ¯ è¯„ä¼°è¯­è¨€: {lang_name}")
            lang_dataset = eval_dataset.filter(
                lambda example: example[self.language_column] == lang_name
            )
            lang_samples = len(lang_dataset)
            #print(f"ğŸ¯ {lang_name} æ ·æœ¬æ•°: {lang_samples}")
            
            if lang_samples > 0:
                lang_metrics = super().evaluate(lang_dataset, ignore_keys, f"eval_{lang_name}")
                
                # ç®€åŒ–æŒ‡æ ‡åç§°ï¼šä» uk/eval_uk_wer æ”¹ä¸º uk/wer
                for k, v in lang_metrics.items():
                    # ç§»é™¤é‡å¤çš„è¯­è¨€å‰ç¼€
                    if k.startswith(f"eval_{lang_name}_"):
                        simplified_key = k.replace(f"eval_{lang_name}_", "")
                    else:
                        simplified_key = k
                    all_lang_metrics[f"{lang_name}/{simplified_key}"] = v
                
                all_lang_metrics[f"{lang_name}/num_samples"] = lang_samples
                
                # ç´¯åŠ æ ·æœ¬æ•°
                total_samples += lang_samples
        
        #print("--------------")
        #print("ç®€åŒ–åçš„æŒ‡æ ‡:", all_lang_metrics)
        #print("è¯­è¨€:", self.languages)
        #print("æ€»æ ·æœ¬æ•°:", total_samples)
        #print("--------------")
        
        # è®¡ç®—åŠ æƒå¹³å‡æŒ‡æ ‡
        weighted_metrics = self._compute_weighted_average(all_lang_metrics, self.languages, total_samples)
        
        # åˆå¹¶æ‰€æœ‰æŒ‡æ ‡
        final_metrics = {**weighted_metrics, **all_lang_metrics}
        return final_metrics

    def _compute_weighted_average(self, lang_metrics, languages, total_samples):
        """è®¡ç®—åŠ æƒå¹³å‡æŒ‡æ ‡ - ä½¿ç”¨ç®€åŒ–åçš„åç§°"""
        weighted_metrics = {}
        
        for metric in ['wer', 'loss']:  # æ ¹æ®æ‚¨å®é™…æœ‰çš„æŒ‡æ ‡è°ƒæ•´
            weighted_sum = 0
            valid_langs = 0
            
            for lang_name in languages:
                lang_metric_key = f"{lang_name}/{metric}"
                lang_samples_key = f"{lang_name}/num_samples"
                
                if lang_metric_key in lang_metrics and lang_samples_key in lang_metrics:
                    lang_value = lang_metrics[lang_metric_key]
                    lang_samples = lang_metrics[lang_samples_key]
                    
                    # åŠ æƒç´¯åŠ 
                    weighted_sum += lang_value * lang_samples
                    valid_langs += 1
            
            # è®¡ç®—åŠ æƒå¹³å‡
            if valid_langs > 0 and total_samples > 0:
                weighted_avg = weighted_sum / total_samples
                weighted_metrics[metric] = weighted_avg
                #print(f"ğŸ¯ åŠ æƒå¹³å‡ {metric}: {weighted_avg} (åŸºäº {valid_langs} ç§è¯­è¨€, {total_samples} æ ·æœ¬)")
                
                wandb.log({f"weighted_avg/{metric}": weighted_avg})
                #print(f"ğŸ“Š å·²è®°å½•åŠ æƒå¹³å‡ {metric} åˆ° wandb: {weighted_avg}")
        weighted_metrics["eval_samples"] = total_samples
        return weighted_metrics



def save_lora_adapter(trainer, adapter_path):
    """ä¿å­˜LoRAé€‚é…å™¨"""
    trainer.model.save_pretrained(adapter_path)

def load_lora_adapter(model, adapter_path):
    """åŠ è½½LoRAé€‚é…å™¨åˆ°åŸºç¡€æ¨¡å‹"""
    
    model = PeftModel.from_pretrained(model, adapter_path)
    return model



def setup_lora_for_ctc(model, lora_config=None,adapter_checkpoint=None):
    """ä¸ºCTCæ¨¡å‹è®¾ç½®LoRAé…ç½®"""
    
    if adapter_checkpoint != None:
        print("load adapter!")
        peft_model = load_lora_adapter(model,adapter_checkpoint)
        return peft_model
    # é»˜è®¤LoRAé…ç½®
    if lora_config is None:
        lora_config = LoraConfig(
            inference_mode=False,
            r=8,  # LoRAç§©
            lora_alpha=32,  # LoRA alpha
            lora_dropout=0.1,  # LoRA dropout
            target_modules=["k_proj", "v_proj", "q_proj", "out_proj"]  # é’ˆå¯¹transformerå±‚çš„æŠ•å½±å±‚
        )
    
    # åº”ç”¨LoRAåˆ°æ¨¡å‹
    peft_model = get_peft_model(model, lora_config)
    peft_model.print_trainable_parameters()
    peft_model.forward = model.forward
    return peft_model

def create_lora_trainer(model, tokenizer, feature_extractor, dataset, training_args, eval_metrics, processor=None, lora_config=None,adapter_checkpoint=None):
    
    # è®¾ç½®LoRA
    model = setup_lora_for_ctc(model, lora_config,adapter_checkpoint)
    
    # åŠ è½½è¯„ä¼°æŒ‡æ ‡
    eval_metrics = {metric: evaluate.load(metric) for metric in eval_metrics}

    def preprocess_logits_for_metrics(logits, labels):
        pred_ids = torch.argmax(logits, dim=-1)
        return pred_ids, labels

    def compute_metrics(pred):
        pred_ids = pred.predictions[0]
        pred.label_ids[pred.label_ids == -100] = tokenizer.pad_token_id

        pred_str = tokenizer.batch_decode(pred_ids)
        #print(f"pred:{pred_str}")
        label_str = tokenizer.batch_decode(pred.label_ids, group_tokens=False)
        #print(f"label:{label_str}")
        metrics = {k: v.compute(predictions=pred_str, references=label_str) for k, v in eval_metrics.items()}

        return metrics

    
    if processor is None:
        processor = AutoProcessor.from_pretrained(training_args.output_dir)
    
    data_collator = DataCollatorCTCWithPadding(processor=processor)

    trainer = MultiLanguageEvaluationTrainer(
        model=model,
        data_collator=data_collator,
        args=training_args,
        compute_metrics=compute_metrics,
        train_dataset=dataset["train"] if training_args.do_train else None,
        eval_dataset=dataset["eval"] if training_args.do_eval else None,
        processing_class=processor,
        preprocess_logits_for_metrics=preprocess_logits_for_metrics,
    )
    
    return trainer



def resize_wav2vec2_ctc_vocab(model, new_vocab_size):
    """è°ƒæ•´Wav2Vec2ForCTCæ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°"""
    
    print(f"è°ƒæ•´Wav2Vec2 CTCæ¨¡å‹è¯æ±‡è¡¨: {model.config.vocab_size} -> {new_vocab_size}")
    
    # è·å–å½“å‰çš„lm_head
    old_lm_head = model.lm_head
    
    # åˆ›å»ºæ–°çš„lm_head
    new_lm_head = torch.nn.Linear(
        old_lm_head.in_features,
        new_vocab_size,
        bias=(old_lm_head.bias is not None)
    )
    
    # å¤åˆ¶æ—§æƒé‡
    old_vocab_size = old_lm_head.out_features
    new_lm_head.weight.data[:old_vocab_size] = old_lm_head.weight.data
    
    # åˆå§‹åŒ–æ–°tokençš„æƒé‡
    if new_vocab_size > old_vocab_size:
        torch.nn.init.normal_(
            new_lm_head.weight.data[old_vocab_size:],
            mean=0.0,
            std=0.02
        )
    
    # å¤åˆ¶biasï¼ˆå¦‚æœæœ‰ï¼‰
    if old_lm_head.bias is not None:
        new_lm_head.bias.data[:old_vocab_size] = old_lm_head.bias.data
        if new_vocab_size > old_vocab_size:
            new_lm_head.bias.data[old_vocab_size:].zero_()
    
    # æ›¿æ¢lm_head
    model.lm_head = new_lm_head
    
    # æ›´æ–°config
    model.config.vocab_size = new_vocab_size
    
    print(f"âœ“ è¯æ±‡è¡¨æ‰©å±•å®Œæˆ: {old_vocab_size} -> {new_vocab_size}")
    return model


def resize_linear_layer(layer, old_size, new_size):
    """è°ƒæ•´çº¿æ€§å±‚çš„è¾“å‡ºå¤§å°"""
    import torch
    import torch.nn as nn

    if isinstance(layer, nn.Linear):
        if new_size > old_size:
            print(f"æ‰©å±•è¾“å‡ºå±‚: {old_size} -> {new_size}")
            
            # ä¿å­˜æ—§æƒé‡
            old_weight = layer.weight.data
            old_bias = layer.bias.data if layer.bias is not None else None
            
            # åˆ›å»ºæ–°å±‚
            new_layer = nn.Linear(
                layer.in_features, 
                new_size, 
                bias=layer.bias is not None
            )
            
            # å¤åˆ¶æ—§æƒé‡
            new_layer.weight.data[:old_size] = old_weight
            
            # åˆå§‹åŒ–æ–°æƒé‡ï¼ˆä½¿ç”¨ä¸åŸå§‹æ¨¡å‹ç›¸åŒçš„åˆå§‹åŒ–ç­–ç•¥ï¼‰
            torch.nn.init.normal_(
                new_layer.weight.data[old_size:], 
                mean=0.0, 
                std=0.02  # å¸¸è§åˆå§‹åŒ–æ ‡å‡†å·®
            )
            
            # å¤„ç†bias
            if old_bias is not None:
                new_layer.bias.data[:old_size] = old_bias
                new_layer.bias.data[old_size:].zero_()
            
            return new_layer

    return layer

def get_new_tokens(vocab_path,existing_dict):
    
    with open(f"{vocab_path}/vocab.json", 'r', encoding='utf-8') as f:
        vocab_dict = json.load(f)
    existing_tokens = set(existing_dict.keys())
    new_tokens = [token for token in vocab_dict.keys() if token not in existing_tokens]
    return new_tokens
