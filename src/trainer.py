# trainer.py
from transformers import Trainer, TrainingArguments
import evaluate
import torch
from collator import DataCollatorCTCWithPadding
from transformers import AutoProcessor
from accelerate import Accelerator
import wandb
class MultiLanguageEvaluationTrainer(Trainer):
    def __init__(self, *args, language_column="language", languages=None, **kwargs):
        #print("ğŸ”§ MultiLanguageEvaluationTrainer.__init__ è¢«è°ƒç”¨")
        super().__init__(*args, **kwargs)
        self.language_column = language_column
        self.languages = languages or []
        #print(f"ğŸ”§ åˆå§‹åŒ–å‚æ•°: language_column={language_column}, languages={languages}")
    
    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix="eval"):
        #print("ğŸ¯ğŸ¯ğŸ¯ MultiLanguageEvaluationTrainer.evaluate è¢«è°ƒç”¨!")

        if eval_dataset is None:
            eval_dataset = self.eval_dataset

        #print(f"ğŸ¯ eval_dataset: {eval_dataset}")
        #print(f"ğŸ¯ metric_key_prefix: {metric_key_prefix}")
        
        # å¦‚æœæœ‰å¤šè¯­è¨€æ•°æ®ï¼Œç›´æ¥è¿›è¡Œå¤šè¯­è¨€è¯„ä¼°å¹¶è®¡ç®—åŠ æƒå¹³å‡
        if eval_dataset is not None and self.language_column in eval_dataset.column_names:
            #print("ğŸ¯ ä½¿ç”¨å¤šè¯­è¨€åŠ æƒè¯„ä¼°...")
            metrics = self._evaluate_with_weighted_average(eval_dataset, ignore_keys, metric_key_prefix)
        else:
            # æ²¡æœ‰è¯­è¨€ä¿¡æ¯ï¼Œå›é€€åˆ°é»˜è®¤è¯„ä¼°
            #print("ğŸ¯ ä½¿ç”¨é»˜è®¤è¯„ä¼°...")
            metrics = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)
        
        #print(f"ğŸ¯ æœ€ç»ˆè¿”å›çš„ metrics: {metrics}")
        return metrics
    
    def _evaluate_with_weighted_average(self, eval_dataset, ignore_keys, metric_key_prefix):
        """ä½¿ç”¨åŠ æƒå¹³å‡è¿›è¡Œå¤šè¯­è¨€è¯„ä¼°"""
        if not self.languages:
            self.languages = list(set(eval_dataset[self.language_column]))
        
        #print(f"ğŸ¯ æ£€æµ‹åˆ°çš„è¯­è¨€: {self.languages}")
        
        all_lang_metrics = {}
        total_samples = 0
        
        # åˆ†åˆ«è¯„ä¼°æ¯ç§è¯­è¨€
        for lang_name in self.languages:
            #print(f"ğŸ¯ è¯„ä¼°è¯­è¨€: {lang_name}")
            lang_dataset = eval_dataset.filter(
                lambda example: example[self.language_column] == lang_name
            )
            lang_samples = len(lang_dataset)
            #print(f"ğŸ¯ {lang_name} æ ·æœ¬æ•°: {lang_samples}")
            
            if lang_samples > 0:
                lang_metrics = super().evaluate(lang_dataset, ignore_keys, f"eval_{lang_name}")
                
                # ç®€åŒ–æŒ‡æ ‡åç§°ï¼šä» uk/eval_uk_wer æ”¹ä¸º uk/wer
                for k, v in lang_metrics.items():
                    # ç§»é™¤é‡å¤çš„è¯­è¨€å‰ç¼€
                    if k.startswith(f"eval_{lang_name}_"):
                        simplified_key = k.replace(f"eval_{lang_name}_", "")
                    else:
                        simplified_key = k
                    all_lang_metrics[f"{lang_name}/{simplified_key}"] = v
                
                all_lang_metrics[f"{lang_name}/num_samples"] = lang_samples
                
                # ç´¯åŠ æ ·æœ¬æ•°
                total_samples += lang_samples
        
        #print("--------------")
        #print("ç®€åŒ–åçš„æŒ‡æ ‡:", all_lang_metrics)
        #print("è¯­è¨€:", self.languages)
        #print("æ€»æ ·æœ¬æ•°:", total_samples)
        #print("--------------")
        
        # è®¡ç®—åŠ æƒå¹³å‡æŒ‡æ ‡
        weighted_metrics = self._compute_weighted_average(all_lang_metrics, self.languages, total_samples)
        
        # åˆå¹¶æ‰€æœ‰æŒ‡æ ‡
        final_metrics = {**weighted_metrics, **all_lang_metrics}
        return final_metrics

    def _compute_weighted_average(self, lang_metrics, languages, total_samples):
        """è®¡ç®—åŠ æƒå¹³å‡æŒ‡æ ‡ - ä½¿ç”¨ç®€åŒ–åçš„åç§°"""
        weighted_metrics = {}
        
        for metric in ['wer', 'loss']:  # æ ¹æ®æ‚¨å®é™…æœ‰çš„æŒ‡æ ‡è°ƒæ•´
            weighted_sum = 0
            valid_langs = 0
            
            for lang_name in languages:
                lang_metric_key = f"{lang_name}/{metric}"
                lang_samples_key = f"{lang_name}/num_samples"
                
                if lang_metric_key in lang_metrics and lang_samples_key in lang_metrics:
                    lang_value = lang_metrics[lang_metric_key]
                    lang_samples = lang_metrics[lang_samples_key]
                    
                    # åŠ æƒç´¯åŠ 
                    weighted_sum += lang_value * lang_samples
                    valid_langs += 1
            
            # è®¡ç®—åŠ æƒå¹³å‡
            if valid_langs > 0 and total_samples > 0:
                weighted_avg = weighted_sum / total_samples
                weighted_metrics[metric] = weighted_avg
                #print(f"ğŸ¯ åŠ æƒå¹³å‡ {metric}: {weighted_avg} (åŸºäº {valid_langs} ç§è¯­è¨€, {total_samples} æ ·æœ¬)")
                
                wandb.log({f"weighted_avg/{metric}": weighted_avg})
                #print(f"ğŸ“Š å·²è®°å½•åŠ æƒå¹³å‡ {metric} åˆ° wandb: {weighted_avg}")
        weighted_metrics["eval_samples"] = total_samples
        return weighted_metrics

def create_trainer(model, tokenizer, feature_extractor, dataset, training_args,eval_metrics, processor=None):
    
    
    #accelerator = Accelerator()
    
    # ç¡®ä¿è®­ç»ƒå‚æ•°æ­£ç¡®è®¾ç½®
    #training_args.ddp_find_unused_parameters = False
    #training_args.remove_unused_columns = False

    # å®šä¹‰è¯„ä»·æŒ‡æ ‡
    #eval_metrics = {metric: evaluate.load(metric) for metric in ["wer"]}
    # Define evaluation metrics during training, *i.e.* word error rate, character error rate
    # cache_dir can be added here
    eval_metrics = {metric: evaluate.load(metric) for metric in eval_metrics}

    def preprocess_logits_for_metrics(logits, labels):
        pred_ids = torch.argmax(logits, dim=-1)
        return pred_ids, labels

    def compute_metrics(pred):
        pred_ids = pred.predictions[0]
        pred.label_ids[pred.label_ids == -100] = tokenizer.pad_token_id

        pred_str = tokenizer.batch_decode(pred_ids)
        print(f"pred:{pred_str}")
        # we do not want to group tokens when computing the metrics
        label_str = tokenizer.batch_decode(pred.label_ids, group_tokens=False)
        print(f"label:{label_str}")
        metrics = {k: v.compute(predictions=pred_str, references=label_str) for k, v in eval_metrics.items()}

        return metrics

    
    if processor == None:
        processor = AutoProcessor.from_pretrained(training_args.output_dir)
    else:
        processor = processor
    
    data_collator = DataCollatorCTCWithPadding(
        processor = processor
    )

    trainer = MultiLanguageEvaluationTrainer(
        model=model,
        data_collator=data_collator,
        args=training_args,
        compute_metrics=compute_metrics,
        train_dataset=dataset["train"] if training_args.do_train else None,
        eval_dataset=dataset["eval"] if training_args.do_eval else None,
        processing_class=processor,
        preprocess_logits_for_metrics=preprocess_logits_for_metrics,
    )

    '''
    trainer = Trainer(
        model=model,
        data_collator=data_collator,
        args=training_args,
        compute_metrics=compute_metrics,
        train_dataset=dataset["train"] if training_args.do_train else None,
        eval_dataset=dataset["eval"] if training_args.do_eval else None,
        processing_class=processor,
        preprocess_logits_for_metrics=preprocess_logits_for_metrics,
    )
    '''
    return trainer