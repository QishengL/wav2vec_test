# preprocess_phonemize.py
import os
import datasets
from phonemizer import phonemize
from phonemizer.separator import Separator
from phonemizer.backend import BACKENDS
from tqdm import tqdm
import time

def preprocess_and_save_phonemized_data(config):
    """
    é¢„å¤„ç†å¹¶ä¿å­˜éŸ³ç´ åŒ–åçš„æ•°æ®
    
    Args:
        config: åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸
            - dataset_name: æ•°æ®é›†åç§° (å¦‚ "fixie-ai/common_voice_17_0")
            - dataset_config_name: è¯­è¨€é…ç½®åˆ—è¡¨ (å¦‚ ["en", "uk", "ru", "tr"])
            - train_split: è®­ç»ƒé›†åˆ†å‰²åç§°
            - test_split: éªŒè¯é›†åˆ†å‰²åç§°  
            - text_column: æ–‡æœ¬åˆ—å (å¦‚ "sentence")
            - cache_dir: ç¼“å­˜ç›®å½•
            - language_phoneme_map: è¯­è¨€åˆ°éŸ³ç´ åç«¯çš„æ˜ å°„
    """
    
    dataset_configs = config["dataset_config_name"]
    if isinstance(dataset_configs, str):
        dataset_configs = [dataset_configs]
    
    language_phoneme_map = config.get("language_phoneme_map", {})
    
    print(f"ğŸš€ å¼€å§‹éŸ³ç´ åŒ–é¢„å¤„ç†...")
    print(f"æ•°æ®é›†: {config['dataset_name']}")
    print(f"è¯­è¨€: {dataset_configs}")
    print(f"ç¼“å­˜ç›®å½•: {config['cache_dir']}")
    
    for config_name in dataset_configs:
        print(f"\n{'='*50}")
        print(f"å¤„ç†è¯­è¨€: {config_name}")
        print(f"{'='*50}")
        
        start_time = time.time()
        
        try:
            # åŠ è½½åŸå§‹æ•°æ®
            print(f"ğŸ“¥ åŠ è½½åŸå§‹æ•°æ®...")
            train_ds = datasets.load_dataset(
                config["dataset_name"],
                config_name,
                split=config["train_split"],
                trust_remote_code=True,
                cache_dir=config["cache_dir"],
            )
            
            
            print(f"âœ… åŠ è½½å®Œæˆ: {len(train_ds)} æ ·æœ¬")
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            continue
        
        # éŸ³ç´ åŒ–å¤„ç†å‡½æ•°
        def phonemize_text(text, language):
            """éŸ³ç´ åŒ–å•ä¸ªæ–‡æœ¬"""
            try:
                phoneme_lang = language_phoneme_map.get(language, language)
                backend = BACKENDS["espeak"](phoneme_lang, language_switch="remove-flags")
                separator = Separator(phone=' ', word="", syllable="")
                
                phonemes = backend.phonemize([text], separator=separator)
                return phonemes[0].strip()
            except Exception as e:
                print(f"âš ï¸ éŸ³ç´ åŒ–å¤±è´¥ {language}: '{text[:50]}...', é”™è¯¯: {e}")
                return text  # å¤±è´¥æ—¶è¿”å›åŸæ–‡æœ¬
        
        def process_dataset(dataset, split_name, language):
            """å¤„ç†æ•´ä¸ªæ•°æ®é›†"""
            phonemized_texts = []
            failed_count = 0
            
            print(f"ğŸ”¤ éŸ³ç´ åŒ– {split_name} æ•°æ®...")
            for i in tqdm(range(len(dataset)), desc=f"Phonemizing {language} {split_name}"):
                text = dataset[i][config["text_column"]]
                phonemized_text = phonemize_text(text, language)
                
                # æ£€æŸ¥æ˜¯å¦éŸ³ç´ åŒ–å¤±è´¥ï¼ˆè¿”å›äº†åŸæ–‡æœ¬ï¼‰
                if phonemized_text == text:
                    failed_count += 1
                
                phonemized_texts.append(phonemized_text)
            
            # æ·»åŠ éŸ³ç´ åŒ–åçš„æ–‡æœ¬åˆ—
            dataset = dataset.add_column("phonemized_text", phonemized_texts)
            
            if failed_count > 0:
                print(f"âš ï¸ {split_name} æœ‰ {failed_count} ä¸ªæ ·æœ¬éŸ³ç´ åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡æœ¬")
            
            return dataset
        
        # å¤„ç†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        try:
            train_ds_phonemized = process_dataset(train_ds, config["train_split"], config_name)
        except Exception as e:
            print(f"âŒ éŸ³ç´ åŒ–å¤„ç†å¤±è´¥: {e}")
            continue
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        output_dir = os.path.join(config["cache_dir"], "phonemized", config_name)
        os.makedirs(output_dir, exist_ok=True)
        
        try:
            print(f"ğŸ’¾ ä¿å­˜éŸ³ç´ åŒ–æ•°æ®åˆ°: {output_dir}")
            train_ds_phonemized.save_to_disk(os.path.join(output_dir, config["train_split"]))
            
            # ä¿å­˜é…ç½®ä¿¡æ¯
            config_info = {
                "dataset_name": config["dataset_name"],
                "language": config_name,
                "phonemized_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                "train_samples": len(train_ds_phonemized),
                "language_phoneme_map": language_phoneme_map.get(config_name, config_name)
            }
            
            with open(os.path.join(output_dir, "config.json"), "w") as f:
                import json
                json.dump(config_info, f, indent=2)
            
            elapsed_time = time.time() - start_time
            print(f"âœ… å®Œæˆ! è€—æ—¶: {elapsed_time:.2f}ç§’")
            print(f"ğŸ“Š è®­ç»ƒé›†: {len(train_ds_phonemized)} æ ·æœ¬")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
    
    print(f"\nğŸ‰ æ‰€æœ‰è¯­è¨€éŸ³ç´ åŒ–é¢„å¤„ç†å®Œæˆ!")

def check_phonemized_data(config):
    """æ£€æŸ¥é¢„éŸ³ç´ åŒ–æ•°æ®æ˜¯å¦å­˜åœ¨"""
    print(f"\nğŸ” æ£€æŸ¥é¢„éŸ³ç´ åŒ–æ•°æ®...")
    
    dataset_configs = config["dataset_config_name"]
    if isinstance(dataset_configs, str):
        dataset_configs = [dataset_configs]
    
    available_languages = []
    missing_languages = []
    
    for config_name in dataset_configs:
        phonemized_dir = os.path.join(config["cache_dir"], "phonemized", config_name)
        train_path = os.path.join(phonemized_dir, "train")

        
        if os.path.exists(train_path) and os.path.exists(eval_path):
            try:
                train_ds = datasets.load_from_disk(train_path)
                available_languages.append(f"{config_name} ({len(train_ds)} train")
            except Exception as e:
                missing_languages.append(f"{config_name} (åŠ è½½å¤±è´¥: {e})")
        else:
            missing_languages.append(config_name)
    
    if available_languages:
        print("âœ… å¯ç”¨çš„é¢„éŸ³ç´ åŒ–æ•°æ®:")
        for lang in available_languages:
            print(f"   - {lang}")
    
    if missing_languages:
        print("âŒ ç¼ºå¤±çš„é¢„éŸ³ç´ åŒ–æ•°æ®:")
        for lang in missing_languages:
            print(f"   - {lang}")
    
    return len(missing_languages) == 0

if __name__ == "__main__":
    # é…ç½®å‚æ•° - æ ¹æ®æ‚¨çš„éœ€æ±‚ä¿®æ”¹
    config = {
        "dataset_name": "fixie-ai/common_voice_17_0",
        "dataset_config_name": ["uk"],  # æ‚¨éœ€è¦çš„è¯­è¨€
        "train_split": "test",
        "text_column": "sentence",  # ç¡®è®¤è¿™æ˜¯æ­£ç¡®çš„æ–‡æœ¬åˆ—å
        "cache_dir": "/mnt/storage/ldl_linguistics/datasets",     # æ‚¨æƒ³è¦ä¿å­˜çš„ç›®å½•
        "language_phoneme_map": {
            "en": "en-us",
            "uk": "uk", 
            "ru": "ru",
            "tr": "tr",    # åœŸè€³å…¶è¯­
            # æ·»åŠ å…¶ä»–è¯­è¨€...
        }
    }
    
    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨é¢„éŸ³ç´ åŒ–æ•°æ®
    if check_phonemized_data(config):
        print("\nğŸ¯ é¢„éŸ³ç´ åŒ–æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†")
        response = input("æ˜¯å¦é‡æ–°å¤„ç†? (y/N): ")
        if response.lower() != 'y':
            exit(0)
    
    # æ‰§è¡ŒéŸ³ç´ åŒ–é¢„å¤„ç†
    preprocess_and_save_phonemized_data(config)